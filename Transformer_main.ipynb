{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_main.ipynb","provenance":[],"authorship_tag":"ABX9TyNQDfHwsMbXUWKUIQePCVlb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sTCgVOC_9KNf","executionInfo":{"status":"ok","timestamp":1640245234117,"user_tz":-60,"elapsed":25259,"user":{"displayName":"Nargiz Umudova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZKXXMfySnPeXln7p6Zjki_3ZrrPe5C_FavcAt=s64","userId":"13937715643671057180"}},"outputId":"956455a1-3a02-4d5d-8c06-72e1efa406a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#Mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","from pandas import read_csv\n","import pandas as pd\n","import datetime as dt\n","import matplotlib\n","from sklearn.linear_model import LinearRegression\n","import numpy as np\n","from sklearn import metrics\n","import matplotlib.pyplot as plt\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# multivariate multi-step encoder-decoder lstm\n","from math import sqrt\n","from numpy import split\n","from numpy import array\n","from pandas import read_csv\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import mean_absolute_percentage_error\n","from sklearn.metrics import mean_absolute_error\n","from matplotlib import pyplot\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers import LSTM\n","from keras.layers import RepeatVector\n","from keras.layers import TimeDistributed\n","from keras.layers import ConvLSTM2D\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","import csv\n","import tensorflow as tf\n","import keras\n","from math import sqrt\n","from numpy import concatenate\n","from matplotlib import pyplot\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","from keras.regularizers import l2\n","from tensorflow.keras import optimizers\n","from keras.callbacks import TerminateOnNaN\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","\n","path1 = '/content/drive/MyDrive/Master_thesis/Datasets/Australia_numpy_removed_night_hours.npy'\n","path2 = '/content/drive/MyDrive/Master_thesis/Datasets/irradiance_AUS(2012-2013).csv'"]},{"cell_type":"code","source":["n_house = 100\n","valid_house = 20 \n","test_house = 20  #for final predictions "],"metadata":{"id":"Mu9zDhGE9NLr","executionInfo":{"status":"ok","timestamp":1640245431870,"user_tz":-60,"elapsed":204,"user":{"displayName":"Nargiz Umudova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZKXXMfySnPeXln7p6Zjki_3ZrrPe5C_FavcAt=s64","userId":"13937715643671057180"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["import random\n","np.random.seed(42)\n","\n","houses = np.loadtxt(path1)\n","shuffle = np.arange(houses.shape[1])\n","np.random.shuffle(shuffle)\n","houses = houses[:,shuffle]\n","irraidance=read_csv(path2, header =0, index_col=0) \n","irraidance=irraidance.drop(\"time\", axis=1)\n","irradiance_array= irraidance.values\n","weath_data = irradiance_array.reshape(irradiance_array.shape[0]*irradiance_array.shape[1])\n","train_val_set = houses[:,:n_house+valid_house]\n","train_val_set = train_val_set.reshape(train_val_set.shape[0]*train_val_set.shape[1], order=\"F\")\n","weath_data_stacked = np.tile(weath_data,n_house+valid_house)\n","data2 = pd.DataFrame(np.column_stack((train_val_set,weath_data_stacked)))\n","test_set = houses[:,-20:] #don't stack. Keep aside for final predictions and loop house by house (add weath_data in each loop)\n","print(shuffle)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x5W8Aj8d9Ooi","executionInfo":{"status":"ok","timestamp":1640245436977,"user_tz":-60,"elapsed":4094,"user":{"displayName":"Nargiz Umudova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZKXXMfySnPeXln7p6Zjki_3ZrrPe5C_FavcAt=s64","userId":"13937715643671057180"}},"outputId":"e1668b91-d2cb-4898-9c7b-a3efc20f7a7f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[281 265 164   9  77 278  93 109   5 173  97 195 184 154  57  60 147 108\n","  63 140 155 104 247  46  42 275 280 116 213 236  17 239  33  24  45   7\n"," 113 194 111  92  75  82 118  76 129 197 210 288 219 178 144 186  84 248\n"," 277  73 244  25 209  59   6 183 185 146  30  22 254  56 237 285 126 228\n"," 283 255 158 225  78  66 296 192 181  19 170  79  90 132  72  15  10 157\n"," 224  68 222  37  16 119 268 269  67 101  69  31 172 148 223 114  18 179\n","  96 165 291  86 246 143 231 124 139 152 204  55 232 182 180 137 251  38\n"," 125 193 167 203 112 264 117 282 273 221 176   2 115 177 175 120 208 259\n"," 127  74  29  83 261 107 287 245 250 294 230  65 196  85 211 159  12  35\n","  28 142 229 279 168  51  95 206 218  41  89 215 136  26 292 141 198   0\n"," 267 271 100 258 253 171  98  36  61 150 234 200 240  11 295 266  27 242\n","   4 122  32 202 162 226 256 138  62 135 128 289   8  70 263  64  44 233\n"," 156  40 123 274 216 153  23 260 110  81 207 212  39 238 290 284 199  14\n","  47  94 262 227 272 201 161  43 217 145 190 220 249   3 105  53 133   1\n"," 131 103  49 163  80 205  34  91  52 241  13  88 166 293 134 286 243  54\n","  50 174 189 297 187 169  58  48 235 252  21 160 276 191 257 149 130 151\n","  99  87 214 121 298  20 188  71 106 270 102]\n"]}]},{"cell_type":"code","source":["# # split a multivariate dataset into train/test sets\n","def split_dataset(data,test_houses):\n","\t# split into standard weeks\n"," train, test = data[0:-365*18*test_houses], data[-365*18*test_houses:]\n","\t# restructure into windows of weekly data\n"," train = array(split(train, len(train)/18))\n"," test = array(split(test, len(test)/18))\n"," print(train.shape,test.shape)\n"," return train, test\n"," \n","# evaluate one or more weekly forecasts against expected values\n","def evaluate_forecasts(actual, predicted):\n","\tscores = list()\n","\t# calculate an RMSE score for each day\n","\tfor i in range(actual.shape[1]):\n","\t\t# calculate mse\n","\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n","\t\t# calculate rmse\n","\t\trmse = sqrt(mse)\n","\t\t# store\n","\t\tscores.append(rmse)\n","\t# calculate overall RMSE\n","\ts = 0\n","\tfor row in range(actual.shape[0]):\n","\t\tfor col in range(actual.shape[1]):\n","\t\t\ts += (actual[row, col] - predicted[row, col])**2\n","\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n","\treturn score, scores\n"," \n","# summarize scores\n","def summarize_scores(name, score, scores):\n","\ts_scores = ', '.join(['%.1f' % s for s in scores])\n","\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n"," \n","# convert history into inputs and outputs\n","def to_supervised(train, n_input, n_out=18):\n","\t# flatten data\n","\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n","\tX, y = list(), list()\n","\tin_start = 0\n","\t# step over the entire history one time step at a time\n","\tfor _ in range(len(data)):\n","\t\t# define the end of the input sequence\n","\t\tin_end = in_start + n_input\n","\t\tout_end = in_end + n_out\n","\t\t# ensure we have enough data for this instance\n","\t\tif out_end <= len(data):\n","\t\t\tX.append(data[in_start:in_end, :])\n","\t\t\ty.append(data[in_end:out_end, 0])\n","\t\t# move along one time step\n","\t\tin_start += 1\n","\treturn array(X), array(y)\n","\n","\n","def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Normalization and Attention\n","    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n","    x = layers.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(x, x)\n","    x = layers.Dropout(dropout)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = layers.LayerNormalization(epsilon=1e-6)(res)\n","    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n","    x = layers.Dropout(dropout)(x)\n","    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n","    return x + res\n","\n"],"metadata":{"id":"q2m0TbxS9TJ-","executionInfo":{"status":"ok","timestamp":1640245470041,"user_tz":-60,"elapsed":191,"user":{"displayName":"Nargiz Umudova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZKXXMfySnPeXln7p6Zjki_3ZrrPe5C_FavcAt=s64","userId":"13937715643671057180"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def build_model(train,n_input,batch_size,epochs,head_size=128,num_heads=4,ff_dim=4,num_transformer_blocks=4,mlp_units=[128],dropout=0.1,mlp_dropout=0):\n","  train_x, train_y = to_supervised(train, n_input=n_input)\n","  input_shape = train_x.shape[1:]\n","  print(\"Train shape =\", train_x.shape)\n","  print(\"Input shape =\", input_shape)\n","  inputs = keras.Input(shape=input_shape)\n","  x = inputs\n","  for _ in range(num_transformer_blocks):\n","     x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","  x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n","  for dim in mlp_units:\n","     x = layers.Dense(dim, activation=\"relu\")(x)\n","     x = layers.Dropout(mlp_dropout)(x)\n","  outputs = layers.Dense(18, activation=\"relu\")(x)\n","  model =  keras.Model(inputs, outputs)\n","  model.compile(loss=\"mse\",optimizer=keras.optimizers.Adam(learning_rate=0.0001),metrics='MeanSquaredError')\n","  #  # stop when loss remains the same for 3 epochs --> passed to the model.fit\n","  early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, mode='min')\n","  #saves the model on drive.\n","  path = '/content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E'+str(epochs)+'_B'+str(batch_size)+'_house_'+str(n_house)+'_input_'+str(n_input)+'.h5'\n","  model_checkpoint = ModelCheckpoint(path, monitor='loss', mode='min', verbose=1, save_best_only=True)\n","  #stop training when loss gets to Nan\n","  call = keras.callbacks.TerminateOnNaN()\n","\t#model.fit(x_train,y_train,validation_split=0.2,epochs=0,batch_size=64,callbacks=callbacks)\n","  model.fit(train_x,train_y,epochs=epochs,batch_size=batch_size,callbacks=[early_stopping,model_checkpoint,call])\n","  return model.summary(),model\n"],"metadata":{"id":"0ilrzuu8-D4J","executionInfo":{"status":"ok","timestamp":1640254399885,"user_tz":-60,"elapsed":45,"user":{"displayName":"Nargiz Umudova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZKXXMfySnPeXln7p6Zjki_3ZrrPe5C_FavcAt=s64","userId":"13937715643671057180"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# make a forecast\n","def forecast(model, history,  n_input):\n","  # flatten data\n","  data = array(history)\n","  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n","  # retrieve last observations for input data\n","  input_x = data[-n_input:, :]\n","  # reshape into [samples, timesteps, rows, cols, channels]\n","  input_x = input_x.reshape((1, n_input, 2))\n","  # forecast the next week\n","  yhat = model.predict(input_x, verbose=0)\n","  # we only want the vector forecast\n","  yhat = yhat[0]\n","  return yhat\n","\n","\n","# define the total days to use as input\n","n_input = 18\n","#define parameters for the model\n","epochs, batch_size = 30,32\n","#data preparation\n","data= data2.values\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","scaled = scaler.fit_transform(data) \n","train,test = split_dataset(data,valid_house)\n","train_scaled, test_scaled = split_dataset(scaled,valid_house)\n","test_X,test_y = to_supervised(test_scaled,n_input)\n","\n","\n","# evaluate model and get scores\n","#model,history_model = build_model(train_scaled, test_X, test_y, epochs, batch_size, n_steps, n_length, n_input)\n","\n","model_summary, model = build_model(train_scaled,n_input,batch_size,epochs)\n","\n","history = [x for x in train_scaled]\n","predictions = list()\n","\n","for i in range(len(test)):\n","  yhat_sequence = forecast(model, history,  n_input)\n","  #inverse scaling manually \n","  test_min = np.min(data[-365*18*valid_house:,0])\n","  test_max = np.max(data[-365*18*valid_house:,0])\n","  yhat_sequence = yhat_sequence * (test_max-test_min)\n","  predictions.append(yhat_sequence)\n","  #add actual observations for next forecasting\n","  history.append(test_scaled[i, :])\n","predictions = array(predictions)\n","score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n","\n","#summarize scores\n","summarize_scores('ConvLSTM', score, scores)\n","# plot scores\n","hours = ['1', '2', '3', '4', '5', '6', '7','8','9','10','11','12','13','14','15','16','17','18']\n","pyplot.plot(hours, scores, marker='o', label='ConvLSTM')\n","pyplot.show()\n","\n","# plot history\n","# pyplot.plot(history_model['loss'], label='train')\n","# pyplot.plot(history_model['val_loss'], label='test')\n","# pyplot.legend()\n","# pyplot.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bu0PKifU-TLH","outputId":"b0db122b-f61c-4351-934a-50160ea775c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(36500, 18, 2) (7300, 18, 2)\n","(36500, 18, 2) (7300, 18, 2)\n","Train shape = (656965, 18, 2)\n","Input shape = (18, 2)\n","Epoch 1/30\n","20530/20531 [============================>.] - ETA: 0s - loss: 0.0049 - mean_squared_error: 0.0049\n","Epoch 00001: loss improved from inf to 0.00494, saving model to /content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E30_B32_house_100_input_18.h5\n","20531/20531 [==============================] - 1852s 90ms/step - loss: 0.0049 - mean_squared_error: 0.0049\n","Epoch 2/30\n","20530/20531 [============================>.] - ETA: 0s - loss: 0.0048 - mean_squared_error: 0.0048\n","Epoch 00002: loss improved from 0.00494 to 0.00480, saving model to /content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E30_B32_house_100_input_18.h5\n","20531/20531 [==============================] - 1844s 90ms/step - loss: 0.0048 - mean_squared_error: 0.0048\n","Epoch 3/30\n","20530/20531 [============================>.] - ETA: 0s - loss: 0.0047 - mean_squared_error: 0.0047\n","Epoch 00003: loss improved from 0.00480 to 0.00473, saving model to /content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E30_B32_house_100_input_18.h5\n","20531/20531 [==============================] - 1862s 91ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n","Epoch 4/30\n","20530/20531 [============================>.] - ETA: 0s - loss: 0.0047 - mean_squared_error: 0.0047\n","Epoch 00004: loss improved from 0.00473 to 0.00467, saving model to /content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E30_B32_house_100_input_18.h5\n","20531/20531 [==============================] - 1847s 90ms/step - loss: 0.0047 - mean_squared_error: 0.0047\n","Epoch 5/30\n","20530/20531 [============================>.] - ETA: 0s - loss: 0.0046 - mean_squared_error: 0.0046\n","Epoch 00005: loss improved from 0.00467 to 0.00460, saving model to /content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E30_B32_house_100_input_18.h5\n","20531/20531 [==============================] - 1871s 91ms/step - loss: 0.0046 - mean_squared_error: 0.0046\n","Epoch 6/30\n","20530/20531 [============================>.] - ETA: 0s - loss: 0.0044 - mean_squared_error: 0.0044\n","Epoch 00006: loss improved from 0.00460 to 0.00442, saving model to /content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E30_B32_house_100_input_18.h5\n","20531/20531 [==============================] - 1877s 91ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n","Epoch 7/30\n","20530/20531 [============================>.] - ETA: 0s - loss: 0.0044 - mean_squared_error: 0.0044\n","Epoch 00007: loss improved from 0.00442 to 0.00438, saving model to /content/drive/MyDrive/Master_thesis/Models/Transformer/LR_0.0001_MLP_128_Head_size_128_E30_B32_house_100_input_18.h5\n","20531/20531 [==============================] - 1875s 91ms/step - loss: 0.0044 - mean_squared_error: 0.0044\n","Epoch 8/30\n","19296/20531 [===========================>..] - ETA: 1:54 - loss: 0.0043 - mean_squared_error: 0.0043"]}]},{"cell_type":"markdown","source":["#Predicting from the saved model"],"metadata":{"id":"HVSpwQDYzd4p"}},{"cell_type":"code","source":["\n","def forecast_daily(model, test,weather,input_length):\n","  all = list()\n","  for house in range(test.shape[1]):\n","    predictions = list()\n","    #iterating over houses and taking each house seperately\n","    data = test[:,house]\n","    #finding min and max values each house seperately\n","    house_max = np.max(data)\n","    house_min = np.min(data)\n","    #combining house with weather data\n","    data = np.column_stack((data,weather))\n","    #scale\n","    scaler = MinMaxScaler()\n","    data = scaler.fit_transform(data)\n","    for hour in range(0,data.shape[0],18):\n","      #looping thourough each house - moving window\n","      input = data[hour:hour+input_length,:]\n","      if input.shape[0]*input.shape[1] < 2* input_length:\n","        break\n","      input = input.reshape((1, n_input, 2))\n","      y_hat = model.predict(input)\n","      y_hat = np.array(y_hat).flatten()\n","      #inverse scaling\n","      y_hat = y_hat * (house_max-house_min)\n","      predictions.append(y_hat)\n","      #save predictions\n","    with open('/content/drive/MyDrive/Master_thesis/Results/Transformer/E20_B64_'+str(house)+'_input_'+str(n_input)+'.csv','a') as nd:\n","      write_n = csv.writer(nd)\n","      write_n.writerows(predictions)\n","    all.append(predictions)\n","  return np.array(all).shape\n","\n","# define the total days to use as input\n","#n_input = 18\n","\n","model = keras.models.load_model('/content/drive/MyDrive/Master_thesis/Models/Transformer/E20_B64_'+str(n_house)+'_input_'+str(90)+'.h5')\n","forecast_daily(model,test_set,weath_data,n_input)"],"metadata":{"id":"X6ritqzi99_1","executionInfo":{"status":"ok","timestamp":1640171554799,"user_tz":-60,"elapsed":402452,"user":{"displayName":"Nargiz Umudova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZKXXMfySnPeXln7p6Zjki_3ZrrPe5C_FavcAt=s64","userId":"13937715643671057180"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"861820a6-f218-45a4-ed16-6483eab6d1b2"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(20, 365, 18)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["#Evaluation"],"metadata":{"id":"CVEa7QHzzlWg"}},{"cell_type":"code","source":["### EVALUATION ### \n","\n","print(\"Shape of the test set = \", test_set.shape)\n","print(\"Shape of the weather set = \", weath_data.shape)\n","# define the number of subsequences and the length of subsequences\n","\n","# define the total days to use as input\n","#n_input = 18\n","# substract hours with input length from the beginning of the data\n","data = test_set[n_input:,:]\n","from statistics import mean\n","\n","RMSEs = list()\n","MAEs=list()\n","\n","\n","for house in range(data.shape[1]):\n","  actual_house = data[:,house]\n","  path ='/content/drive/MyDrive/Master_thesis/Results/Transformer/E20_B64_'+str(house)+'_input_'+str(n_input)+'.csv'\n","  pred_house = read_csv(path,header=None)[:-1]\n","  pred_house = np.array(pred_house)\n","  #print(actual_house.shape,pred_house.shape)\n","  \n","  row_pred = 0\n","  for hours in range(0,data.shape[0]-n_input,18):\n","  #for hours in range(1):\n","    actual =  actual_house[hours: 18+hours]\n","    pred = pred_house[row_pred,:]\n"," \n","    rmse = sqrt(mean_squared_error(actual,pred))\n","    RMSEs.append(rmse)\n","    mae= mean_absolute_error(actual,pred) \n","    MAEs.append(mae)\n","    row_pred+=1\n","\n","print(\"RMSE\",mean(RMSEs),\"Shape of RMSEs = \", np.array(RMSEs).shape)\n","print(\"MAE\",mean(MAEs),\"Shape of MAEs = \", np.array(MAEs).shape)\n"],"metadata":{"id":"vg-ABn2IznZw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640171710859,"user_tz":-60,"elapsed":3516,"user":{"displayName":"Nargiz Umudova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgZKXXMfySnPeXln7p6Zjki_3ZrrPe5C_FavcAt=s64","userId":"13937715643671057180"}},"outputId":"9601fbde-c410-443a-e2a5-16f5bfd5044b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of the test set =  (6570, 20)\n","Shape of the weather set =  (6570,)\n","RMSE 0.3765129430079039 Shape of RMSEs =  (7260,)\n","MAE 0.327015023379639 Shape of MAEs =  (7260,)\n"]}]}]}