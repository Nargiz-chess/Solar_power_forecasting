{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_AUSTRALIA_10_days.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nargiz-chess/Solar_forecasting/blob/main/LSTM_AUSTRALIA_10_days.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3AQ-9lpcQ8Z",
        "outputId": "2154ba18-7ee1-415f-b413-006f3bf3e4b8"
      },
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import matplotlib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# multivariate multi-step encoder-decoder lstm\n",
        "from math import sqrt\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import csv\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJNlmqynMbr"
      },
      "source": [
        "#Parameters to change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5wgVAELmXhk"
      },
      "source": [
        "#number of houses\n",
        "n_house = 50\n",
        "#number of days\n",
        "n_days = 10\n",
        "#minumum delta for early stopping \n",
        "early_stop_delta = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGgQ_tIBWKVx"
      },
      "source": [
        "# SUBSET: draw random houses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU90sqde1Hzr"
      },
      "source": [
        "path1 = '/content/drive/MyDrive/Master_thesis/Datasets/Australia_numpy_removed_night_hours.npy'\n",
        "path2 = '/content/drive/MyDrive/Master_thesis/Datasets/irradiance_AUS(2012-2013).csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WznTYYoPXLKU"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "subset_col=[]\n",
        "while True:\n",
        "  r=random.randint(0,298)\n",
        "  if r not in subset_col:\n",
        "    subset_col.append(r)\n",
        "  if len(subset_col) ==n_house:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NmTq44pW4Ec",
        "outputId": "56c903ba-44d2-4d89-e029-47190a9ffee1"
      },
      "source": [
        "array = np.loadtxt(path1)\n",
        "print(\"Shape before reshape\",np.shape(array))\n",
        "\n",
        "array = array[:,subset_col]\n",
        "print(array.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before reshape (6570, 299)\n",
            "(6570, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt2ENQcFYm5g",
        "outputId": "b874580c-6462-4114-8f64-faa070f487e7"
      },
      "source": [
        "AUS_data = array.reshape(array.shape[0]*array.shape[1], order=\"F\")\n",
        "print(\"Shape after reshape\",np.shape(AUS_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after reshape (328500,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFLxPylFW_fo",
        "outputId": "0a9640fe-8669-4de9-d29a-d6885cce9908"
      },
      "source": [
        "df2=read_csv(path2, header =0, index_col=0) \n",
        "df2.shape\n",
        "df2=df2.drop(\"time\", axis=1)\n",
        "array2 = df2.values\n",
        "weath_data = array2.reshape(array2.shape[0]*array2.shape[1])\n",
        "print(\"Shape of Aus data = \", AUS_data.shape, \"\\n\",\"Shape of weather data = \", weath_data.shape )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Aus data =  (328500,) \n",
            " Shape of weather data =  (6570,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzzRaCDGWJ45",
        "outputId": "49b496c1-3401-4f9a-c335-6a6820aa3e71"
      },
      "source": [
        "#stacking irradiance as many times as there are houses in the dataset to match with each house in the datset\n",
        "\n",
        "weath_data_stacked = np.tile(weath_data,n_house)\n",
        "weath_data_stacked.shape\n",
        "data= np.column_stack((AUS_data,weath_data_stacked))\n",
        "data.shape\n",
        "#data = np.nan_to_num(data)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(328500, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D7JWciFZN5j"
      },
      "source": [
        "from numpy import split\n",
        "from numpy import array\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# split a multivariate dataset into train/test sets\n",
        "def split_dataset(data):\n",
        "  # split \n",
        "  train, test = data[:-18], data[-18:]\n",
        "  return train, test\n",
        "\n",
        "def restructure_windows(train,test): \n",
        "  # restructure into windows\n",
        "  train = array(split(train, len(train)/18))\n",
        "  test = array(split(test, len(test)/18))\n",
        "  return train, test\n",
        "\n",
        "def scaling(train,test):\n",
        "  #scaling values\n",
        "  scalers={}\n",
        "\n",
        "  scaled_train = []\n",
        "  for i in range(train.shape[1]):\n",
        "    scaler = MinMaxScaler(feature_range=(0,1))\n",
        "    s_s = scaler.fit_transform(train[:,i].reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+ str(i)] = scaler\n",
        "    scaled_train.append(s_s)\n",
        "  \n",
        "  scaled_test = []\n",
        "  for i in range(train.shape[1]):\n",
        "    scaler = scalers['scaler_'+str(i)]\n",
        "    s_s = scaler.transform(test[:,i].reshape(-1,1))\n",
        "    s_s=np.reshape(s_s,len(s_s))\n",
        "    scalers['scaler_'+str(i)] = scaler\n",
        "    scaled_test.append(s_s)\n",
        "  \n",
        "  scaled_train = array(scaled_train)\n",
        "  scaled_train=scaled_train.transpose()\n",
        "\n",
        "  scaled_test = array(scaled_test)\n",
        "  scaled_test=scaled_test.transpose()\n",
        "  return scaled_train,scaled_test,scalers\n",
        "\n",
        "# evaluate hours forecasts against expected values\n",
        "def evaluate_forecasts(actual, predicted):\n",
        "  scores = list()\n",
        "  # calculate an RMSE score for each day\n",
        "  for i in range(actual.shape[1]):\n",
        "    # calculate mse\n",
        "    mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
        "    # calculate rmse\n",
        "    rmse = sqrt(mse)\n",
        "    # store\n",
        "    scores.append(rmse)\n",
        "  # calculate overall RMSE\n",
        "  s = 0\n",
        "  for row in range(actual.shape[0]):\n",
        "    for col in range(actual.shape[1]):\n",
        "      s += (actual[row, col] - predicted[row, col])**2\n",
        "  score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
        "  return score, scores\n",
        "\n",
        "# summarize scores\n",
        "def summarize_scores(name, score, scores):\n",
        "  s_scores = ', '.join(['%.1f' % s for s in scores])\n",
        "  print('%s: [%.3f] %s' % (name, score, s_scores))\n",
        "  \n",
        "# convert history into inputs and outputs\n",
        "def to_supervised(train, n_input, n_out=18):\n",
        "  # flatten data\n",
        "  data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
        "  X, y = list(), list()\n",
        "  in_start = 0\n",
        "  # step over the entire history one time step at a time\n",
        "  for _ in range(len(data)):\n",
        "    # define the end of the input sequence\n",
        "    in_end = in_start + n_input\n",
        "    out_end = in_end + n_out\n",
        "    # ensure we have enough data for this instance\n",
        "    # Here we are dividing the data differently because we are working with a multivariate dataset (It contains more variables) \n",
        "    if out_end < len(data):\n",
        "      X.append(data[in_start:in_end, :])\n",
        "      y.append(data[in_end:out_end, 0])\n",
        "    # move along one time step\n",
        "    in_start += 1\n",
        "  return array(X), array(y)\n",
        "\n",
        "# train the model\n",
        "def build_model(train, n_input):\n",
        "  # prepare data\n",
        "  train_x, train_y = to_supervised(train, n_input)\n",
        "  # define parameters\n",
        "  verbose, epochs, batch_size = 1, 20, 54\n",
        "  n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
        "  # # reshape into subsequences [samples, timesteps, rows, cols, channels]\n",
        "  # train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n",
        "  # # reshape output into [samples, timesteps, features]\n",
        "  # train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
        "\n",
        "  # stop when loss remains the same for 2 epochs --> passed to the model.fit\n",
        "  early_stopping = EarlyStopping(monitor='loss', patience=2,min_delta=early_stop_delta,restore_best_weights=True)\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(100, activation='relu', input_shape=(n_timesteps, n_features)))\n",
        "  model.add(RepeatVector(n_outputs))\n",
        "  model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "  #model.add(LSTM(20, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(100, activation='relu')))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "  #saves the model on drive. Because of the loop (CV), one more parameter was passed (nth_house)\n",
        "  path = '/content/drive/MyDrive/Master_thesis/Models/LSTM_house_'+str(n_days)+'_days_'+str(n_house)+'_houses_ALL_test.h5'\n",
        "  model_checkpoint = ModelCheckpoint(path, monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
        "  # fit network\n",
        "  model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,callbacks=[early_stopping,model_checkpoint], verbose=verbose, shuffle=False)\n",
        "  return model\n",
        "\n",
        "\n",
        "# make a forecast\n",
        "def forecast(model, history,  n_input):\n",
        "  # flatten data\n",
        "  data = array(history)\n",
        "  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
        "  # retrieve last observations for input data\n",
        "  input_x = data[-n_input:, :]\n",
        "  # reshape into [1, n_input, n]\n",
        "  input_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
        "  # forecast the next \n",
        "  yhat = model.predict(input_x, verbose=0)\n",
        "  # we only want the vector forecast\n",
        "  yhat = yhat[0]\n",
        "  return yhat\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(train_scaled, test, n_input,test_scaled,scaler_fitted): \n",
        "  # fit model\n",
        "  model = build_model(train_scaled,  n_input)\n",
        "  history = [x for x in train_scaled]\n",
        "  # walk-forward validation over each 5 days\n",
        "  predictions = list()\n",
        "  for i in range(len(test_scaled)):\n",
        "    # predict the next day\n",
        "    yhat_sequence = forecast(model, history, n_input)\n",
        "    # store the predictions\n",
        "    predictions.append(yhat_sequence)\n",
        "    # get real observation and add to history for predicting the next \n",
        "    history.append(test_scaled[i, :])\n",
        "  # evaluate predictions \n",
        "  predictions = array(predictions) \n",
        "  #print(\"shape predictions not inverse scaled\", predictions.shape)\n",
        "  #print(\"predictions\",predictions)\n",
        "  #inverse predictions \n",
        "  predictions =  predictions[:,:,0]\n",
        "  predictions_inverse_scaled = scaler_fitted.inverse_transform(predictions)\n",
        "  # sometimes there are very small negative numbers. These will be treated as zero\n",
        "  predictions_inverse_scaled = np.abs(predictions_inverse_scaled)\n",
        "  #evaluate with test data and inverse_scaled predicitions\n",
        "  score, scores = evaluate_forecasts(test[:, :, 0], predictions_inverse_scaled)\n",
        "  #print(\"prediction shape =\",predictions_inverse_scaled.shape)\n",
        "  #print(\"predictions_inverse_scaled\", predictions_inverse_scaled)\n",
        "  #print(\"Shape of test[:,:,0]\", test[:, :, 0].shape)\n",
        "  return score, scores,predictions_inverse_scaled\n",
        "\n",
        "def random_cv(array1,nth_house,length):\n",
        "  house = array1[(nth_house-1)*length: nth_house*length]\n",
        "  before_house = array1[:(nth_house-1)*length]\n",
        "  after_house = array1[nth_house*length:]\n",
        "  return np.concatenate((before_house,after_house,house))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCsodmbpZ3GW",
        "outputId": "e386c13b-c222-46c4-8c7d-5e644c35cad1"
      },
      "source": [
        "#for i in random_houses:\n",
        "for i in range(3,n_house+1):\n",
        "  data = random_cv(data,i,6570)\n",
        "  #data = pd.DataFrame(data, columns = [\"solar_output\",\"irradiance\"])\n",
        "  # split into train and test\n",
        "  train, test = split_dataset(data)\n",
        "  #scaling train,test\n",
        "  train_scaled,test_scaled, scalers = scaling(train,test)\n",
        "  #restructure test into windows without scaling. Non-scaled test is needed to evaluate forecast (passing it to \"evaluate_model\" function)\n",
        "  _,unscaled_test = restructure_windows(train,test)\n",
        "  #restructure into windows\n",
        "  train_structured,test_structured = restructure_windows(train_scaled,test_scaled)\n",
        "  #define the number of subsequences and the length of subsequences\n",
        "  #n_days, n_length = 5, 18\n",
        "  n_length = 18\n",
        "  # define the total days to use as input\n",
        "  n_input = n_length * n_days\n",
        "  scaler = scalers['scaler_0']\n",
        "  score, scores,predictions = evaluate_model(train_structured, unscaled_test, n_input,test_structured,scaler)\n",
        "  with open('/content/drive/MyDrive/Master_thesis/Results/LSTM_Overall_RMSEs_AUS_'+str(n_house)+'_houses_'+str(n_days)+'_days_ALL_test.csv','a') as fd:\n",
        "    write = csv.writer(fd)\n",
        "    write.writerow([score])\n",
        "  with open('/content/drive/MyDrive/Master_thesis/Results/LSTM_RMSEs_per_hour_AUS_'+str(n_house)+'_houses_'+str(n_days)+'_days_ALL_test.csv','a') as nd:\n",
        "    write_n = csv.writer(nd)\n",
        "    write_n.writerow(scores)\n",
        "  # summarize scores\n",
        "  summarize_scores('LSTM', score, scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 00001: loss improved from inf to 0.00312, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_3.h5\n",
            "6080/6080 [==============================] - 775s 127ms/step - loss: 0.0031\n",
            "Epoch 2/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0027\n",
            "Epoch 00002: loss improved from 0.00312 to 0.00272, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_3.h5\n",
            "6080/6080 [==============================] - 779s 128ms/step - loss: 0.0027\n",
            "Epoch 3/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0026\n",
            "Epoch 00003: loss improved from 0.00272 to 0.00264, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_3.h5\n",
            "6080/6080 [==============================] - 782s 129ms/step - loss: 0.0026\n",
            "prediction shape = (1, 18)\n",
            "predictions_inverse_scaled [[0.06386777 0.08751021 0.04949504 0.02671802 0.13856272 0.28509736\n",
            "  0.43368042 0.53116834 0.5299972  0.4105725  0.27737617 0.16517258\n",
            "  0.06885704 0.0091289  0.02236917 0.03148739 0.0238338  0.00162899]]\n",
            "Shape of test[:,:,0] (1, 18)\n",
            "ConvLSTM: [0.148] 0.1, 0.1, 0.0, 0.0, 0.1, 0.1, 0.4, 0.1, 0.0, 0.1, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
            "Epoch 1/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 00001: loss improved from inf to 0.00305, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_4.h5\n",
            "6080/6080 [==============================] - 781s 128ms/step - loss: 0.0031\n",
            "Epoch 2/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0027\n",
            "Epoch 00002: loss improved from 0.00305 to 0.00266, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_4.h5\n",
            "6080/6080 [==============================] - 796s 131ms/step - loss: 0.0027\n",
            "Epoch 3/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0025\n",
            "Epoch 00003: loss improved from 0.00266 to 0.00250, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_4.h5\n",
            "6080/6080 [==============================] - 792s 130ms/step - loss: 0.0025\n",
            "prediction shape = (1, 18)\n",
            "predictions_inverse_scaled [[0.14421104 0.12812966 0.12982371 0.15818353 0.20284037 0.25346422\n",
            "  0.30555904 0.3493487  0.40507898 0.38754347 0.27996266 0.16991727\n",
            "  0.06628244 0.01559666 0.05120472 0.06358834 0.07990474 0.08805659]]\n",
            "Shape of test[:,:,0] (1, 18)\n",
            "ConvLSTM: [0.231] 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.9, 0.3, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.1\n",
            "Epoch 1/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 00001: loss improved from inf to 0.00309, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_5.h5\n",
            "6080/6080 [==============================] - 770s 126ms/step - loss: 0.0031\n",
            "Epoch 2/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0026\n",
            "Epoch 00002: loss improved from 0.00309 to 0.00260, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_5.h5\n",
            "6080/6080 [==============================] - 777s 128ms/step - loss: 0.0026\n",
            "Epoch 3/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0025\n",
            "Epoch 00003: loss improved from 0.00260 to 0.00247, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_5.h5\n",
            "6080/6080 [==============================] - 771s 127ms/step - loss: 0.0025\n",
            "prediction shape = (1, 18)\n",
            "predictions_inverse_scaled [[0.06995843 0.07639859 0.02726008 0.16219227 0.30989838 0.4743445\n",
            "  0.5915299  0.5887641  0.5278704  0.38782728 0.24290872 0.14528409\n",
            "  0.09544715 0.05571939 0.02862119 0.01481111 0.01194234 0.01788982]]\n",
            "Shape of test[:,:,0] (1, 18)\n",
            "ConvLSTM: [0.094] 0.1, 0.1, 0.0, 0.1, 0.1, 0.1, 0.1, 0.0, 0.3, 0.1, 0.0, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0\n",
            "Epoch 1/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0031\n",
            "Epoch 00001: loss improved from inf to 0.00313, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_6.h5\n",
            "6080/6080 [==============================] - 697s 114ms/step - loss: 0.0031\n",
            "Epoch 2/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0025\n",
            "Epoch 00002: loss improved from 0.00313 to 0.00253, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_6.h5\n",
            "6080/6080 [==============================] - 693s 114ms/step - loss: 0.0025\n",
            "Epoch 3/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0024\n",
            "Epoch 00003: loss improved from 0.00253 to 0.00244, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_6.h5\n",
            "6080/6080 [==============================] - 687s 113ms/step - loss: 0.0024\n",
            "prediction shape = (1, 18)\n",
            "predictions_inverse_scaled [[0.02616277 0.01298529 0.06232617 0.14405559 0.2045396  0.2643403\n",
            "  0.28536302 0.2969686  0.27682328 0.23143163 0.16843036 0.10586851\n",
            "  0.06300837 0.03768824 0.02316365 0.0182496  0.02547233 0.04378897]]\n",
            "Shape of test[:,:,0] (1, 18)\n",
            "ConvLSTM: [0.101] 0.0, 0.0, 0.1, 0.1, 0.2, 0.1, 0.1, 0.3, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0\n",
            "Epoch 1/10\n",
            "6080/6080 [==============================] - ETA: 0s - loss: 0.0030\n",
            "Epoch 00001: loss improved from inf to 0.00305, saving model to /content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_10_days_50_houses_7.h5\n",
            "6080/6080 [==============================] - 731s 119ms/step - loss: 0.0030\n",
            "Epoch 2/10\n",
            " 575/6080 [=>............................] - ETA: 10:58 - loss: 0.0024"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL0zQAy4zoxh"
      },
      "source": [
        "path = '/content/drive/MyDrive/Master_thesis/Results/Overall_RMSEs_AUS_50_houses_10_days.csv'\n",
        "rmse40=read_csv(path, header =None, index_col=False)\n",
        "rmse40.mean()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}