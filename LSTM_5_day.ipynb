{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_5 day.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nargiz-chess/Solar_power_forecasting/blob/main/LSTM_5_day.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz3xQpd9UJL1",
        "outputId": "48290e60-2494-49f7-9462-9916a5f74f97"
      },
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import matplotlib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# multivariate multi-step encoder-decoder lstm\n",
        "from math import sqrt\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from math import sqrt\n",
        "from numpy import concatenate\n",
        "from matplotlib import pyplot\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.regularizers import l2\n",
        "from tensorflow.keras import optimizers\n",
        "from keras.callbacks import TerminateOnNaN\n",
        "\n",
        "\n",
        "path1 = '/content/drive/MyDrive/Master_thesis/Datasets/Australia_numpy_removed_night_hours.npy'\n",
        "path2 = '/content/drive/MyDrive/Master_thesis/Datasets/irradiance_AUS(2012-2013).csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qijG442JW8ek"
      },
      "source": [
        "n_house = 100\n",
        "valid_house = 20 \n",
        "test_house = 20  #for final predictions "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWTA2aiiW-pd"
      },
      "source": [
        "import random\n",
        "np.random.seed(42)\n",
        "\n",
        "houses = np.loadtxt(path1)\n",
        "shuffle = np.arange(houses.shape[1])\n",
        "np.random.shuffle(shuffle)\n",
        "houses = houses[:,shuffle]\n",
        "irraidance=read_csv(path2, header =0, index_col=0) \n",
        "irraidance=irraidance.drop(\"time\", axis=1)\n",
        "irradiance_array= irraidance.values\n",
        "weath_data = irradiance_array.reshape(irradiance_array.shape[0]*irradiance_array.shape[1])\n",
        "train_val_set = houses[:,:n_house+valid_house]\n",
        "train_val_set = train_val_set.reshape(train_val_set.shape[0]*train_val_set.shape[1], order=\"F\")\n",
        "weath_data_stacked = np.tile(weath_data,n_house+valid_house)\n",
        "data2 = pd.DataFrame(np.column_stack((train_val_set,weath_data_stacked)))\n",
        "test_set = houses[:,-20:]#don't stack. Keep aside for final predictions and loop house by house (add weath_data in each loop)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEHTTZpdXIOf"
      },
      "source": [
        "# # split a univariate dataset into train/test sets\n",
        "def split_dataset(data,test_houses):\n",
        "\t# split into standard weeks\n",
        " train, test = data[0:-365*18*test_houses], data[-365*18*test_houses:]\n",
        "\t# restructure into windows of weekly data\n",
        " train = array(split(train, len(train)/18))\n",
        " test = array(split(test, len(test)/18))\n",
        " print(train.shape,test.shape)\n",
        " return train, test\n",
        " \n",
        "# evaluate one or more weekly forecasts against expected values\n",
        "def evaluate_forecasts(actual, predicted):\n",
        "\tscores = list()\n",
        "\t# calculate an RMSE score for each day\n",
        "\tfor i in range(actual.shape[1]):\n",
        "\t\t# calculate mse\n",
        "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
        "\t\t# calculate rmse\n",
        "\t\trmse = sqrt(mse)\n",
        "\t\t# store\n",
        "\t\tscores.append(rmse)\n",
        "\t# calculate overall RMSE\n",
        "\ts = 0\n",
        "\tfor row in range(actual.shape[0]):\n",
        "\t\tfor col in range(actual.shape[1]):\n",
        "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
        "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
        "\treturn score, scores\n",
        " \n",
        "# summarize scores\n",
        "def summarize_scores(name, score, scores):\n",
        "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
        "\tprint('%s: [%.3f] %s' % (name, score, s_scores))\n",
        " \n",
        "# convert history into inputs and outputs\n",
        "def to_supervised(train, n_input, n_out=18):\n",
        "\t# flatten data\n",
        "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
        "\tX, y = list(), list()\n",
        "\tin_start = 0\n",
        "\t# step over the entire history one time step at a time\n",
        "\tfor _ in range(len(data)):\n",
        "\t\t# define the end of the input sequence\n",
        "\t\tin_end = in_start + n_input\n",
        "\t\tout_end = in_end + n_out\n",
        "\t\t# ensure we have enough data for this instance\n",
        "\t\tif out_end <= len(data):\n",
        "\t\t\tX.append(data[in_start:in_end, :])\n",
        "\t\t\ty.append(data[in_end:out_end, 0])\n",
        "\t\t# move along one time step\n",
        "\t\tin_start += 1\n",
        "\treturn array(X), array(y)\n",
        " \n",
        "# train the model\n",
        "def build_model(train, n_input,test_X,test_y,epochs, batch_size ):\n",
        " # prepare data\n",
        " train_x, train_y = to_supervised(train, n_input)\n",
        " # define parameters\n",
        " n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
        " # reshape output into [samples, timesteps, features]\n",
        " train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
        " test_y =  test_y.reshape((test_y.shape[0], test_y.shape[1], 1))\n",
        " # stop when loss remains the same for 3 epochs --> passed to the model.fit\n",
        " early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        " #saves the model on drive.\n",
        " path = '/content/drive/MyDrive/Master_thesis/Models/LSTM/B64_E30_HL50_house_'+str(n_house)+'_input_'+str(n_input)+'.h5'\n",
        " model_checkpoint = ModelCheckpoint(path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        " #stop training when loss gets to Nan\n",
        " call = keras.callbacks.TerminateOnNaN()\n",
        " # define model\n",
        " model = Sequential()\n",
        " model.add(LSTM(100, activation='relu', input_shape=(n_timesteps, n_features)))\n",
        " model.add(RepeatVector(n_outputs))\n",
        " model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        " model.add(TimeDistributed(Dense(100, activation='relu')))\n",
        " model.add(TimeDistributed(Dense(1)))\n",
        " #remove line below when you activate the second layer above\n",
        " #model.add(Dense(n_outputs))\n",
        " model.compile(loss='mse', optimizer='adam')\n",
        " # fit network\n",
        " history_model = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping,model_checkpoint,call],validation_data = (test_X,test_y))\n",
        "# path2 = '/content/drive/MyDrive/Master_thesis/Models/LSTM/Loss_Val_loss_house_1_hiddenlayer'+str(n_house)+'_input_'+str(n_input)\n",
        "#  with open(path2+'.csv','a') as nd:\n",
        "#      write_n = csv.writer(nd)\n",
        "#      write_n.writerow([history_model.history['loss'],history_model.history['val_loss']])\n",
        " return model,history_model.history\n",
        " \n",
        "# make a forecast\n",
        "def forecast(model, history, n_input):\n",
        "\t# flatten data\n",
        "\tdata = array(history)\n",
        "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
        "\t# retrieve last observations for input data\n",
        "\tinput_x = data[-n_input:, :]\n",
        "\t# reshape into [1, n_input, n]\n",
        "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
        "\t# forecast the next week\n",
        "\tyhat = model.predict(input_x, verbose=0)\n",
        "\t# we only want the vector forecast\n",
        "\tyhat = yhat[0]\n",
        "\treturn yhat\n",
        " \n",
        "n_input = 90\n",
        "epochs, batch_size = 30,64\n",
        "#data preparation\n",
        "data= data2.values\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(data) \n",
        "train,test = split_dataset(data,valid_house)\n",
        "train_scaled, test_scaled = split_dataset(scaled,valid_house)\n",
        "test_X,test_y = to_supervised(test_scaled,n_input)\n",
        "\n",
        "\n",
        "# evaluate model and get scores\n",
        "model,history_model = build_model(train_scaled, n_input,test_X,test_y,epochs, batch_size)\n",
        "history = [x for x in train_scaled]\n",
        "predictions = list()\n",
        "\n",
        "for i in range(len(test)):\n",
        "  yhat_sequence = forecast(model, history, n_input)\n",
        "  #inverse scaling manually \n",
        "  test_min = np.min(data[-365*18*valid_house:,0])\n",
        "  test_max = np.max(data[-365*18*valid_house:,0])\n",
        "  yhat_sequence = yhat_sequence * (test_max-test_min)\n",
        "  predictions.append(yhat_sequence)\n",
        "  #add actual observations for next forecasting\n",
        "  history.append(test_scaled[i, :])\n",
        "predictions = array(predictions)\n",
        "score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
        "\n",
        "# summarize scores\n",
        "summarize_scores('lstm', score, scores)\n",
        "# plot scores\n",
        "hours = ['1', '2', '3', '4', '5', '6', '7','8','9','10','11','12','13','14','15','16','17','18']\n",
        "pyplot.plot(hours, scores, marker='o', label='lstm')\n",
        "pyplot.show()\n",
        "\n",
        "# plot history\n",
        "pyplot.plot(history_model['loss'], label='train')\n",
        "pyplot.plot(history_model['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xPXYwWfYPX3"
      },
      "source": [
        "#Predictions based on unseen test set "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MQWemOEgKan"
      },
      "source": [
        "##Forecasting with a daily moving window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXusa_RugO9S",
        "outputId": "ab4ed271-378b-470f-cb4d-0217c31af0bc"
      },
      "source": [
        "def forecast_daily(model, test,weather,input_length):\n",
        "  all = list()\n",
        "  for house in range(test.shape[1]):\n",
        "    predictions = list()\n",
        "    #iterating over houses and taking each house seperately\n",
        "    data = test[:,house]\n",
        "    #finding min and max values each house seperately\n",
        "    house_max = np.max(data)\n",
        "    house_min = np.min(data)\n",
        "    #combining house with weather data\n",
        "    data = np.column_stack((data,weather))\n",
        "    #scale\n",
        "    scaler = MinMaxScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "    for hour in range(0,data.shape[0],18):\n",
        "      #looping thourough each house - moving window\n",
        "      input = data[hour:hour+input_length,:]\n",
        "      if input.shape[0]*input.shape[1] < 2* input_length:\n",
        "        break\n",
        "      input = input.reshape(1,input_length,2)\n",
        "      y_hat = model.predict(input)\n",
        "      y_hat = np.array(y_hat).flatten()\n",
        "      #inverse scaling\n",
        "      y_hat = y_hat * (house_max-house_min)\n",
        "      predictions.append(y_hat)\n",
        "      #save predictions\n",
        "    path_results = '/content/drive/MyDrive/Master_thesis/Results/LSTM/Copy models/Copy B64_E30_HL50_'+str(house)+'_input_'+str(n_input)+'.csv'\n",
        "    with open(path_results,'a') as nd:\n",
        "      write_n = csv.writer(nd)\n",
        "      write_n.writerows(predictions)\n",
        "    all.append(predictions)\n",
        "  return np.array(all).shape\n",
        "\n",
        "n_input=90\n",
        "model = keras.models.load_model('/content/drive/MyDrive/Master_thesis/Models/LSTM/Copy of B64_E30_HL50_house_'+str(n_house)+'_input_'+str(n_input)+'.h5')\n",
        "forecast_daily(model,test_set,weath_data,n_input)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 361, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNRwj2jDgQBZ"
      },
      "source": [
        "### EVALUATION ###\n",
        "\n",
        "print(\"Shape of the test set = \", test_set.shape)\n",
        "print(\"Shape of the weather set = \", weath_data.shape)\n",
        "n_input = 90\n",
        "# substract hours with input length from the beginning of the data\n",
        "data = test_set[n_input:,:]\n",
        "from statistics import mean\n",
        "\n",
        "RMSEs = list()\n",
        "MAEs=list()\n",
        "\n",
        "for house in range(data.shape[1]):\n",
        "  actual_house = data[:,house]\n",
        "  path ='/content/drive/MyDrive/Master_thesis/Results/LSTM/Copy models/Copy B64_E30_HL50_'+str(house)+'_input_'+str(n_input)+'.csv'\n",
        "  pred_house = read_csv(path,header=None)[:-1]\n",
        "  pred_house = np.array(pred_house)\n",
        "  #print(actual_house.shape,pred_house.shape)\n",
        "  \n",
        "  row_pred = 0\n",
        "  for hours in range(0,data.shape[0]-n_input,18):\n",
        "  #for hours in range(1):\n",
        "    actual =  actual_house[hours: 18+hours]\n",
        "    pred = pred_house[row_pred,:]\n",
        " \n",
        "    rmse = sqrt(mean_squared_error(actual,pred))\n",
        "    RMSEs.append(rmse)\n",
        "    mae= mean_absolute_error(actual,pred) \n",
        "    MAEs.append(mae)\n",
        "    row_pred+=1\n",
        "\n",
        "print(\"RMSE\",mean(RMSEs),\"Shape of RMSEs = \", np.array(RMSEs).shape)\n",
        "print(\"MAE\",mean(MAEs),\"Shape of MAEs = \", np.array(MAEs).shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}