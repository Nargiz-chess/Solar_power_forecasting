{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvLSTM_AUSTRALIA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6NsrcnKXvrE6SSixMJGR2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nargiz-chess/Solar_forecasting/blob/main/ConvLSTM_AUSTRALIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3AQ-9lpcQ8Z",
        "outputId": "84555dd4-8031-4725-e9af-a1414bc85d99"
      },
      "source": [
        "#Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from pandas import read_csv\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "import matplotlib\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# multivariate multi-step encoder-decoder lstm\n",
        "from math import sqrt\n",
        "from numpy import split\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import ConvLSTM2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5uZwVb9ctK_"
      },
      "source": [
        "path1 = '/content/drive/MyDrive/Master_thesis/Datasets/Australia_numpy_removed_night_hours.npy'\n",
        "path2 = '/content/drive/MyDrive/Master_thesis/Datasets/irradiance_AUS(2012-2013).csv'"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWgorXM6cw2H",
        "outputId": "e85364d6-ab34-4efb-8a70-a23265f005a1"
      },
      "source": [
        "array = np.loadtxt(path1)\n",
        "print(\"Shape before reshape\",np.shape(array))\n",
        "AUS_data = array.reshape(array.shape[0]*array.shape[1], order=\"F\")\n",
        "print(\"Shape after reshape\",np.shape(AUS_data))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before reshape (6570, 299)\n",
            "Shape after reshape (1964430,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4zRYj1Bc3Kk",
        "outputId": "03d55259-6844-416e-8796-caa6119f9471"
      },
      "source": [
        "df2=read_csv(path2, header =0, index_col=0) \n",
        "df2.shape\n",
        "df2=df2.drop(\"time\", axis=1)\n",
        "array2 = df2.values\n",
        "weath_data = array2.reshape(array2.shape[0]*array2.shape[1])\n",
        "print(\"Shape of Aus data = \", AUS_data.shape, \"\\n\",\"Shape of weather data = \", weath_data.shape )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of Aus data =  (1964430,) \n",
            " Shape of weather data =  (6570,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqhFZ2yEdClK",
        "outputId": "a9c32ec6-fe95-4736-e2ac-aa67603a4760"
      },
      "source": [
        "#stacking irradiance as many times as there are houses in the dataset to match with each house in the datset\n",
        "\n",
        "# weath_data_stacked = np.tile(weath_data,299)\n",
        "# weath_data_stacked.shape\n",
        "# data= np.column_stack((AUS_data,weath_data_stacked))\n",
        "# data.shape\n",
        "# data = np.nan_to_num(data)\n",
        "# data = pd.DataFrame(data=data)\n",
        "data.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1964430, 2)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yax1mSOJCk7O"
      },
      "source": [
        "from numpy import split\n",
        "from numpy import array\n",
        "\n",
        "\n",
        "# split a univariate dataset into train/test sets\n",
        "def split_dataset(data):\n",
        "  # split into standard weeks\n",
        "  train, test = data[:-90], data[-90:]\n",
        "  # restructure into windows of weekly data\n",
        "  train = array(split(train, len(train)/18))\n",
        "  test = array(split(test, len(test)/18))\n",
        "  return train, test\n",
        "\n",
        "# evaluate one or more weekly forecasts against expected values\n",
        "def evaluate_forecasts(actual, predicted):\n",
        "  scores = list()\n",
        "  # calculate an RMSE score for each day\n",
        "  for i in range(actual.shape[1]):\n",
        "    # calculate mse\n",
        "    mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
        "    # calculate rmse\n",
        "    rmse = sqrt(mse)\n",
        "    # store\n",
        "    scores.append(rmse)\n",
        "  # calculate overall RMSE\n",
        "  s = 0\n",
        "  for row in range(actual.shape[0]):\n",
        "    for col in range(actual.shape[1]):\n",
        "      s += (actual[row, col] - predicted[row, col])**2\n",
        "  score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
        "  return score, scores\n",
        "\n",
        "# summarize scores\n",
        "def summarize_scores(name, score, scores):\n",
        "  s_scores = ', '.join(['%.1f' % s for s in scores])\n",
        "  print('%s: [%.3f] %s' % (name, score, s_scores))\n",
        "  \n",
        "# convert history into inputs and outputs\n",
        "def to_supervised(train, n_input, n_out=18):\n",
        "  # flatten data\n",
        "  data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
        "  X, y = list(), list()\n",
        "  in_start = 0\n",
        "  # step over the entire history one time step at a time\n",
        "  for _ in range(len(data)):\n",
        "    # define the end of the input sequence\n",
        "    in_end = in_start + n_input\n",
        "    out_end = in_end + n_out\n",
        "    # ensure we have enough data for this instance\n",
        "    # Here we are dividing the data differently because we are working with a multivariate dataset (It contains more variables) \n",
        "    if out_end < len(data):\n",
        "      X.append(data[in_start:in_end, :])\n",
        "      y.append(data[in_end:out_end, 0])\n",
        "    # move along one time step\n",
        "    in_start += 1\n",
        "  return array(X), array(y)\n",
        "\n",
        "# train the model\n",
        "def build_model(train, n_steps, n_length, n_input):\n",
        "  # prepare data\n",
        "  train_x, train_y = to_supervised(train, n_input)\n",
        "  # define parameters\n",
        "  verbose, epochs, batch_size = 1, 1, 54\n",
        "  n_features, n_outputs = train_x.shape[2], train_y.shape[1]\n",
        "  # reshape into subsequences [samples, timesteps, rows, cols, channels]\n",
        "  train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n",
        "  # reshape output into [samples, timesteps, features]\n",
        "  train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(ConvLSTM2D(filters=20, kernel_size=(1,3), activation='relu',input_shape=(n_steps, 1, n_length, n_features)))\n",
        "  model.add(Flatten())\n",
        "  model.add(RepeatVector(n_outputs))\n",
        "  model.add(LSTM(20, activation='relu', return_sequences=True))\n",
        "  #model.add(LSTM(20, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(100, activation='relu')))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  # fit network\n",
        "  model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        "  return model\n",
        "\n",
        "# make a forecast\n",
        "def forecast(model, history, n_steps, n_length, n_input):\n",
        "  # flatten data\n",
        "  data = array(history)\n",
        "  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
        "  # retrieve last observations for input data\n",
        "  input_x = data[-n_input:, :]\n",
        "  # reshape into [samples, timesteps, rows, cols, channels]\n",
        "  input_x = input_x.reshape((1, n_steps, 1, n_length, 2))\n",
        "  # forecast the next week\n",
        "  yhat = model.predict(input_x, verbose=0)\n",
        "  # we only want the vector forecast\n",
        "  yhat = yhat[0]\n",
        "  return yhat\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(train, test, n_steps, n_length, n_input):\n",
        "  # fit model\n",
        "  model = build_model(train, n_steps, n_length, n_input)\n",
        "  # history is a list of weekly data\n",
        "  history = [x for x in train]\n",
        "  # walk-forward validation over each week\n",
        "  predictions = list()\n",
        "  for i in range(len(test)):\n",
        "    # predict the next day\n",
        "    yhat_sequence = forecast(model, history, n_steps, n_length, n_input)\n",
        "    # store the predictions\n",
        "    predictions.append(yhat_sequence)\n",
        "    # get real observation and add to history for predicting the next week\n",
        "    history.append(test[i, :])\n",
        "  # evaluate predictions days for each week\n",
        "  predictions = array(predictions)\n",
        "  score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
        "  return score, scores,predictions\n",
        "\n",
        "\n",
        "# # split into train and test\n",
        "# train, test = split_dataset(data.values)\n",
        "# # define the number of subsequences and the length of subsequences\n",
        "# n_steps, n_length = 5, 18\n",
        "# # define the total days to use as input\n",
        "# n_input = n_length * n_steps\n",
        "# score, scores,predictions = evaluate_model(train, test, n_steps, n_length, n_input)\n",
        "# # summarize scores\n",
        "# summarize_scores('lstm', score, scores)\n",
        "# # # plot scores\n",
        "\n",
        "# #hours = [x for x in range(1,19)]\n",
        "# hours = [str(x) for x in range(1,19)]\n",
        "# pyplot.plot(hours, scores, marker='o', label='lstm')\n",
        "# pyplot.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLMDWWesBI50",
        "outputId": "7762e91d-6cb3-48a8-993f-2de989a38211"
      },
      "source": [
        "def cross_val (array, samples_division):\n",
        "  array = np.concatenate((array[-samples_division:],array[:-samples_division] ))\n",
        "  return array\n",
        "\n",
        "scores_cv = []\n",
        "for i in range(1):\n",
        "  data = cross_val(data,6570)\n",
        "  # split into train and test\n",
        "  train, test = split_dataset(data)\n",
        "  # define the number of subsequences and the length of subsequences\n",
        "  n_steps, n_length = 5, 18\n",
        "  # define the total days to use as input\n",
        "  n_input = n_length * n_steps\n",
        "  score, scores,predictions = evaluate_model(train, test, n_steps, n_length, n_input)\n",
        "  # summarize scores\n",
        "  summarize_scores('lstm', score, scores)\n",
        "  scores_cv.append(scores)\n",
        "\n",
        "print(scores_cv)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36375/36375 [==============================] - 1741s 48ms/step - loss: 2.5366\n",
            "lstm: [0.287] 0.3, 0.3, 0.3, 0.3, 0.3, 0.2, 0.1, 0.1, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3\n",
            "[0. 0.]\n",
            "36375/36375 [==============================] - 1706s 47ms/step - loss: 1.9398\n",
            "lstm: [0.061] 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
            "[0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGgQ_tIBWKVx"
      },
      "source": [
        "# SUBSET: random 100 houses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU90sqde1Hzr"
      },
      "source": [
        "path1 = '/content/drive/MyDrive/Master_thesis/Datasets/Australia_numpy_removed_night_hours.npy'\n",
        "path2 = '/content/drive/MyDrive/Master_thesis/Datasets/irradiance_AUS(2012-2013).csv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WznTYYoPXLKU",
        "outputId": "50eb8738-43bf-4d84-80e8-8fa0327fbab2"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "subset_col = [random.randint(0, 298) for p in range(0, 100)]\n",
        "print(subset_col)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[57, 12, 140, 125, 114, 71, 52, 279, 44, 216, 16, 15, 47, 111, 119, 258, 13, 287, 101, 279, 214, 112, 229, 142, 3, 81, 216, 174, 142, 79, 110, 172, 52, 47, 194, 49, 183, 176, 135, 22, 235, 274, 63, 193, 40, 282, 150, 185, 295, 98, 35, 23, 116, 148, 40, 119, 51, 194, 142, 232, 186, 83, 189, 181, 107, 136, 36, 87, 273, 125, 83, 236, 194, 138, 285, 112, 166, 28, 117, 16, 161, 205, 137, 33, 108, 290, 161, 108, 255, 202, 234, 73, 135, 71, 126, 287, 275, 134, 219, 298]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NmTq44pW4Ec",
        "outputId": "52553bdd-092a-49c8-c4c8-5540b5b9402e"
      },
      "source": [
        "array = np.loadtxt(path1)\n",
        "print(\"Shape before reshape\",np.shape(array))\n",
        "\n",
        "array = array[:,subset_col]\n",
        "print(array.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before reshape (6570, 299)\n",
            "(6570, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt2ENQcFYm5g",
        "outputId": "a1a387b8-e949-4791-d947-aa18b3de5b96"
      },
      "source": [
        "AUS_data = array.reshape(array.shape[0]*array.shape[1], order=\"F\")\n",
        "print(\"Shape after reshape\",np.shape(AUS_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape after reshape (657000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFLxPylFW_fo",
        "outputId": "df40719f-9107-4112-9c57-e6e59c294df6"
      },
      "source": [
        "df2=read_csv(path2, header =0, index_col=0) \n",
        "df2.shape\n",
        "df2=df2.drop(\"time\", axis=1)\n",
        "array2 = df2.values\n",
        "weath_data = array2.reshape(array2.shape[0]*array2.shape[1])\n",
        "print(\"Shape of Aus data = \", AUS_data.shape, \"\\n\",\"Shape of weather data = \", weath_data.shape )\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Aus data =  (657000,) \n",
            " Shape of weather data =  (6570,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzzRaCDGWJ45",
        "outputId": "ee71e372-9c10-49b2-ff38-6192794b0802"
      },
      "source": [
        "#stacking irradiance as many times as there are houses in the dataset to match with each house in the datset\n",
        "\n",
        "weath_data_stacked = np.tile(weath_data,100)\n",
        "weath_data_stacked.shape\n",
        "data= np.column_stack((AUS_data,weath_data_stacked))\n",
        "data.shape\n",
        "data = np.nan_to_num(data)\n",
        "data = pd.DataFrame(data=data)\n",
        "data.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(657000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D7JWciFZN5j"
      },
      "source": [
        "from numpy import split\n",
        "from numpy import array\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# split a univariate dataset into train/test sets\n",
        "def split_dataset(data):\n",
        "  # split into standard weeks\n",
        "  train, test = data[:-90], data[-90:]\n",
        "  # restructure into windows of weekly data\n",
        "  train = array(split(train, len(train)/18))\n",
        "  test = array(split(test, len(test)/18))\n",
        "  return train, test\n",
        "\n",
        "# evaluate one or more weekly forecasts against expected values\n",
        "def evaluate_forecasts(actual, predicted):\n",
        "  scores = list()\n",
        "  # calculate an RMSE score for each day\n",
        "  for i in range(actual.shape[1]):\n",
        "    # calculate mse\n",
        "    mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
        "    # calculate rmse\n",
        "    rmse = sqrt(mse)\n",
        "    # store\n",
        "    scores.append(rmse)\n",
        "  # calculate overall RMSE\n",
        "  s = 0\n",
        "  for row in range(actual.shape[0]):\n",
        "    for col in range(actual.shape[1]):\n",
        "      s += (actual[row, col] - predicted[row, col])**2\n",
        "  score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
        "  return score, scores\n",
        "\n",
        "# summarize scores\n",
        "def summarize_scores(name, score, scores):\n",
        "  s_scores = ', '.join(['%.1f' % s for s in scores])\n",
        "  print('%s: [%.3f] %s' % (name, score, s_scores))\n",
        "  \n",
        "# convert history into inputs and outputs\n",
        "def to_supervised(train, n_input, n_out=18):\n",
        "  # flatten data\n",
        "  data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
        "  X, y = list(), list()\n",
        "  in_start = 0\n",
        "  # step over the entire history one time step at a time\n",
        "  for _ in range(len(data)):\n",
        "    # define the end of the input sequence\n",
        "    in_end = in_start + n_input\n",
        "    out_end = in_end + n_out\n",
        "    # ensure we have enough data for this instance\n",
        "    # Here we are dividing the data differently because we are working with a multivariate dataset (It contains more variables) \n",
        "    if out_end < len(data):\n",
        "      X.append(data[in_start:in_end, :])\n",
        "      y.append(data[in_end:out_end, 0])\n",
        "    # move along one time step\n",
        "    in_start += 1\n",
        "  return array(X), array(y)\n",
        "\n",
        "# train the model\n",
        "def build_model(train, n_steps, n_length, n_input,nth_house):\n",
        "  # prepare data\n",
        "  train_x, train_y = to_supervised(train, n_input)\n",
        "  # define parameters\n",
        "  verbose, epochs, batch_size = 1, 1, 54\n",
        "  n_features, n_outputs = train_x.shape[2], train_y.shape[1]\n",
        "  # reshape into subsequences [samples, timesteps, rows, cols, channels]\n",
        "  train_x = train_x.reshape((train_x.shape[0], n_steps, 1, n_length, n_features))\n",
        "  # reshape output into [samples, timesteps, features]\n",
        "  train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
        "\n",
        "  # stop when loss remains the same for 2 epochs --> passed to the model.fit\n",
        "  early_stopping = EarlyStopping(monitor='loss', patience=2,min_delta=0.001,restore_best_weights=True)\n",
        "\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(ConvLSTM2D(filters=20, kernel_size=(1,3), activation='relu',input_shape=(n_steps, 1, n_length, n_features)))\n",
        "  model.add(Flatten())\n",
        "  model.add(RepeatVector(n_outputs))\n",
        "  #model.add(LSTM(100, activation='relu', return_sequences=True))\n",
        "  model.add(LSTM(20, activation='relu', return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(100, activation='relu')))\n",
        "  model.add(TimeDistributed(Dense(1)))\n",
        "  model.compile(loss='mse', optimizer='adam')\n",
        "  path = '/content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_'+str(nth_house)+\".h5\"\n",
        "  model_checkpoint = ModelCheckpoint(path, monitor='loss', mode='min', verbose=1, save_best_only=True)\n",
        "  # fit network\n",
        "  model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,callbacks=[early_stopping,model_checkpoint], verbose=verbose)\n",
        "  return model\n",
        "\n",
        "# make a forecast\n",
        "def forecast(model, history, n_steps, n_length, n_input):\n",
        "  # flatten data\n",
        "  data = array(history)\n",
        "  data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
        "  # retrieve last observations for input data\n",
        "  input_x = data[-n_input:, :]\n",
        "  # reshape into [samples, timesteps, rows, cols, channels]\n",
        "  input_x = input_x.reshape((1, n_steps, 1, n_length, 2))\n",
        "  # forecast the next week\n",
        "  yhat = model.predict(input_x, verbose=0)\n",
        "  # we only want the vector forecast\n",
        "  yhat = yhat[0]\n",
        "  return yhat\n",
        "\n",
        "# evaluate a single model\n",
        "def evaluate_model(train, test, n_steps, n_length, n_input,nth_house):\n",
        "  # fit model\n",
        "  model = build_model(train, n_steps, n_length, n_input,nth_house)\n",
        "  # history is a list of weekly data\n",
        "  history = [x for x in train]\n",
        "  # walk-forward validation over each week\n",
        "  predictions = list()\n",
        "  for i in range(len(test)):\n",
        "    # predict the next day\n",
        "    yhat_sequence = forecast(model, history, n_steps, n_length, n_input)\n",
        "    # store the predictions\n",
        "    predictions.append(yhat_sequence)\n",
        "    # get real observation and add to history for predicting the next week\n",
        "    history.append(test[i, :])\n",
        "  # evaluate predictions days for each week\n",
        "  predictions = array(predictions)\n",
        "  score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
        "  return score, scores,predictions\n",
        "\n",
        "# def cross_val (array, samples_division):\n",
        "#   array = np.concatenate((array[-samples_division:],array[:-samples_division] ))\n",
        "#   return array\n",
        "\n",
        "# scores_cv = []\n",
        "# for i in range(2):\n",
        "#   data = cross_val(data,6570)\n",
        "#   # split into train and test\n",
        "#   train, test = split_dataset(data)\n",
        "#   # define the number of subsequences and the length of subsequences\n",
        "#   n_steps, n_length = 5, 18\n",
        "#   # define the total days to use as input\n",
        "#   n_input = n_length * n_steps\n",
        "#   score, scores,predictions = evaluate_model(train, test, n_steps, n_length, n_input)\n",
        "#   # summarize scores\n",
        "#   summarize_scores('lstm', score, scores)\n",
        "#   scores_cv.append(scores)\n",
        "\n",
        "# print(scores_cv)\n",
        "# # split into train and test\n",
        "# train, test = split_dataset(data.values)\n",
        "# # define the number of subsequences and the length of subsequences\n",
        "# n_steps, n_length = 5, 18\n",
        "# # define the total days to use as input\n",
        "# n_input = n_length * n_steps\n",
        "# score, scores,predictions = evaluate_model(train, test, n_steps, n_length, n_input)\n",
        "# # summarize scores\n",
        "# summarize_scores('lstm', score, scores)\n",
        "# # # # plot scores\n",
        "\n",
        "# #hours = [x for x in range(1,19)]\n",
        "# hours = [str(x) for x in range(1,19)]\n",
        "# pyplot.plot(hours, scores, marker='o', label='lstm')\n",
        "# pyplot.show()\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_LjQRj1aB9X"
      },
      "source": [
        "#CV randomly selecting 10 houses for testing, afterwards averaging scores (rmse)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekW8TyLjaMFF",
        "outputId": "9fd952c1-469b-4a2d-dfd5-9284157b4541"
      },
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "random_houses = [random.randint(0, 99) for p in range(0, 10)]\n",
        "print(random_houses)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81, 14, 3, 94, 35, 31, 28, 17, 94, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCsodmbpZ3GW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "51569f5c-7ebd-4baf-dff8-abd2b4fc1516"
      },
      "source": [
        "def random_cv(array1,nth_house,length):\n",
        "  #print(\"array\",array1)\n",
        "  house = array1[(nth_house-1)*length: nth_house*length]\n",
        "  #print(\"house\",house)\n",
        "  before_house = array1[:(nth_house-1)*length]\n",
        "  #print(\"before\",before_house)\n",
        "  after_house = array1[nth_house*length:]\n",
        "  #print(\"after\",after_house)\n",
        "  return np.concatenate((before_house,after_house,house))\n",
        "\n",
        "scores_cv = []\n",
        "#for i in random_houses:\n",
        "for i in range(1):\n",
        "  data = random_cv(data,i,6570)\n",
        "  # split into train and test\n",
        "  train, test = split_dataset(data)\n",
        "  # define the number of subsequences and the length of subsequences\n",
        "  n_steps, n_length = 5, 18\n",
        "  # define the total days to use as input\n",
        "  n_input = n_length * n_steps\n",
        "  score, scores,predictions = evaluate_model(train, test, n_steps, n_length, n_input,i)\n",
        "  # summarize scores\n",
        "  summarize_scores('ConvLSTM', score, scores)\n",
        "  scores_cv.append(scores)\n",
        "\n",
        "print(np.array(scores_cv).shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  800/48298 [..............................] - ETA: 39:08 - loss: 14.7023"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1b51db77ade2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;31m# define the total days to use as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_length\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0;31m# summarize scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0msummarize_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ConvLSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9dd72516c887>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(train, test, n_steps, n_length, n_input, nth_house)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnth_house\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;31m# fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnth_house\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0;31m# history is a list of weekly data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9dd72516c887>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(train, n_steps, n_length, n_input, nth_house)\u001b[0m\n\u001b[1;32m     85\u001b[0m   \u001b[0mmodel_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNqmJMhPxpwC",
        "outputId": "8a787a3b-bdda-4abb-eb1e-5a34ca78c7f0"
      },
      "source": [
        "import numpy as np\n",
        "print(np.array(scores_cv).shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2To5sxHfWWR"
      },
      "source": [
        "def cross_val (array_house, samples_division,house_length):\n",
        "  start_index = samples_division * house_length\n",
        "  print(start_index)\n",
        "  end_index = (samples_division+1)* house_length\n",
        "  print(end_index)\n",
        "  test_subset = array_house.iloc[start_index:end_index,axis=0]\n",
        "  print(test_subset)\n",
        "  array_house = np.delete(array_house,test_subset)\n",
        "  print(array_house)\n",
        "  array_house = np.concatenate((array_house,test_subset ))\n",
        "  return array_house\n",
        "\n",
        "cross_val(data,10,6570)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oOwWNN3McxK",
        "outputId": "5f66d234-d632-4c57-d541-8c5c39ceb26f"
      },
      "source": [
        "path = '/content/drive/MyDrive/Master_thesis/Models/ConvLSTM_house_'+str(5)+\".h5\"\n",
        "print(type(path))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    }
  ]
}